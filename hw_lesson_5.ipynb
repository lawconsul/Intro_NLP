{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNdqOpBvOHv7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 2000\n",
    "max_len = 40\n",
    "num_classes = 1\n",
    "\n",
    "# Training\n",
    "epochs = 20\n",
    "batch_size = 512\n",
    "print_batch_n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhwOTPn7M_85"
   },
   "source": [
    "Задание из 2-х частей.\n",
    "Берем отызывы за лето (из архива с материалами или предыдущего занятия)\n",
    "1. Учим conv сеть для классификации - выбить auc выше 0.95\n",
    "2. Предобучаем word2vec и его эмбединга инициализируем сетку, как влияет на качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "df = pd.read_excel('summer.xls', sep='\\t')\n",
    "# создаем df\n",
    "DF = pd.DataFrame()\n",
    "DF['text'] = df['Content']\n",
    "DF['label']  = (df['Rating']>3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = DF[:10000]\n",
    "df_val = DF[10000:20000]\n",
    "df_test = DF[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/radik/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/Users/radik/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/radik/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"\\sне\", \"не\", txt)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "df_val['text'] = df_val['text'].apply(preprocess_text)\n",
    "df_test['text'] = df_test['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4o9QgmWI3Pw"
   },
   "outputs": [],
   "source": [
    "train_corpus = \" \".join(df_train[\"text\"])\n",
    "train_corpus = train_corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10183,
     "status": "ok",
     "timestamp": 1590650264825,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -240
    },
    "id": "Hed2ySbwJH6B",
    "outputId": "66a75988-b4e3-45aa-a483-adbdc766db03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "tokens = word_tokenize(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJ8T0fwYJYJX"
   },
   "source": [
    "Отфильтруем данные\n",
    "\n",
    "и соберём в корпус N наиболее частых токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXOLVK1tJLT8"
   },
   "outputs": [],
   "source": [
    "tokens_filtered = [word for word in tokens if word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qCQH5nIJoiB"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "dist = FreqDist(tokens_filtered)\n",
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1590650396521,
     "user": {
      "displayName": "Roman Zakharov",
      "photoUrl": "",
      "userId": "18255168926005506833"
     },
     "user_tz": -240
    },
    "id": "bRQ-6wwjJrGo",
    "outputId": "ba8ebf9e-c6aa-4703-b54f-39b561e33493"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['приложение',\n",
       " 'удобно',\n",
       " 'работать',\n",
       " 'удобный',\n",
       " 'отлично',\n",
       " 'хороший',\n",
       " 'нравиться',\n",
       " 'отличный',\n",
       " 'быстро',\n",
       " 'супер']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_filtered_top[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}\n",
    "# vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def text_to_sequence(text, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "    padding = [0]*(maxlen-len(result))\n",
    "    return padding + result[-maxlen:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"text\"]], dtype=np.int32)\n",
    "x_test = np.asarray([text_to_sequence(text, max_len) for text in df_test[\"text\"]], dtype=np.int32)\n",
    "x_val = np.asarray([text_to_sequence(text, max_len) for text in df_val[\"text\"]], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 40)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping  \n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = 2\n",
    "y_train = keras.utils.to_categorical(df_train[\"label\"], num_classes)\n",
    "y_val = keras.utils.to_categorical(df_val[\"label\"], num_classes)\n",
    "y_test = keras.utils.to_categorical(df_test[\"label\"], num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "y_train_ = (y_train[:,0]>0.5).astype(int)\n",
    "y_val_ = (y_val[:,0]>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 2s 248us/step - loss: 0.5909 - accuracy: 0.7904 - val_loss: 0.4465 - val_accuracy: 0.8500\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 2s 199us/step - loss: 0.4097 - accuracy: 0.8382 - val_loss: 0.3492 - val_accuracy: 0.8520\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 2s 200us/step - loss: 0.3079 - accuracy: 0.8628 - val_loss: 0.2465 - val_accuracy: 0.9020\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 2s 200us/step - loss: 0.2178 - accuracy: 0.9134 - val_loss: 0.2153 - val_accuracy: 0.9100\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 2s 201us/step - loss: 0.1731 - accuracy: 0.9317 - val_loss: 0.2021 - val_accuracy: 0.9150\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 2s 201us/step - loss: 0.1462 - accuracy: 0.9451 - val_loss: 0.2073 - val_accuracy: 0.9100\n"
     ]
    }
   ],
   "source": [
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 57us/step\n",
      "\n",
      "\n",
      "Test score: 0.25340595951080325\n",
      "Test accuracy: 0.8925999999046326\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659/659 [==============================] - 0s 208us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(x_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (results[:,0]>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ = (y_test[:,0]>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(659,)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8597270943575718"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test_, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 2s 221us/step - loss: 0.8857 - accuracy: 0.5769 - val_loss: 0.5039 - val_accuracy: 0.8500\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 1s 134us/step - loss: 0.4845 - accuracy: 0.8371 - val_loss: 0.4221 - val_accuracy: 0.8510\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 1s 139us/step - loss: 0.4022 - accuracy: 0.8381 - val_loss: 0.3586 - val_accuracy: 0.8510\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 1s 142us/step - loss: 0.3346 - accuracy: 0.8403 - val_loss: 0.2927 - val_accuracy: 0.8535\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 1s 140us/step - loss: 0.2609 - accuracy: 0.8684 - val_loss: 0.2515 - val_accuracy: 0.8955\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 1s 142us/step - loss: 0.2128 - accuracy: 0.9057 - val_loss: 0.2294 - val_accuracy: 0.9085\n",
      "Epoch 7/20\n",
      "9000/9000 [==============================] - 1s 147us/step - loss: 0.1836 - accuracy: 0.9286 - val_loss: 0.2381 - val_accuracy: 0.9115\n",
      "10000/10000 [==============================] - 0s 37us/step\n",
      "\n",
      "\n",
      "Test score: 0.30180133876800536\n",
      "Test accuracy: 0.8903499841690063\n",
      "659/659 [==============================] - 1s 807us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8710535703668469"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(32))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])\n",
    "\n",
    "score = model.evaluate(x_val, y_val, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "results = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "res = (results[:,0]>0.5).astype(int)\n",
    "\n",
    "y_test_ = (y_test[:,0]>0.5).astype(int)\n",
    "\n",
    "roc_auc_score(y_test_, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 2000\n",
    "max_len = 40\n",
    "num_classes = 1\n",
    "\n",
    "# Training\n",
    "epochs = 40\n",
    "batch_size = 512\n",
    "print_batch_n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ = (y_train[:,0]>0.5).astype(int)\n",
    "y_val_ = (y_val[:,0]>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 3s 372us/step - loss: 0.5621 - accuracy: 0.8337 - val_loss: 0.4173 - val_accuracy: 0.8500\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 1s 125us/step - loss: 0.4024 - accuracy: 0.8373 - val_loss: 0.3433 - val_accuracy: 0.8520\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 1s 131us/step - loss: 0.3033 - accuracy: 0.8689 - val_loss: 0.2480 - val_accuracy: 0.8950\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 1s 133us/step - loss: 0.2163 - accuracy: 0.9106 - val_loss: 0.2118 - val_accuracy: 0.9080\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 1s 135us/step - loss: 0.1750 - accuracy: 0.9311 - val_loss: 0.2000 - val_accuracy: 0.9140\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 1s 142us/step - loss: 0.1515 - accuracy: 0.9413 - val_loss: 0.2023 - val_accuracy: 0.9150\n",
      "10000/10000 [==============================] - 0s 37us/step\n",
      "\n",
      "\n",
      "Test score: 0.24682916493415832\n",
      "Test accuracy: 0.8931000232696533\n",
      "659/659 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8747250133777276"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "# model.add(Dense(32))\n",
    "# model.add(Activation(\"relu\"))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "history = model.fit(x_train, y_train_,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])\n",
    "\n",
    "score = model.evaluate(x_val, y_val_, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "results = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "res = (results[:,0]>0.5).astype(int)\n",
    "\n",
    "y_test_ = (y_test[:,0]>0.5).astype(int)\n",
    "\n",
    "roc_auc_score(y_test_, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы:\n",
    "Максимальное значение ROC-AUC, которого удалось достичь - 0.875.\n",
    "Что предпринималось:\n",
    "    1. Изменялось кол-во слоев CNN\n",
    "    2. y_train и y_val были приведены к одному классу (>0.5)\n",
    "    3. Изменяло кол-во итераций\n",
    "    4. Выбор Активации между Relu и Sigmoid\n",
    "    5. Выбор loss между 'binary_crossentropy' и 'categorical_crossentropy'\n",
    "    6. Даже Применялись слои LSTM для RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "9000/9000 [==============================] - 8s 930us/step - loss: 0.5015 - accuracy: 0.8123 - val_loss: 0.3781 - val_accuracy: 0.8500\n",
      "Epoch 2/20\n",
      "9000/9000 [==============================] - 8s 847us/step - loss: 0.3526 - accuracy: 0.8509 - val_loss: 0.2974 - val_accuracy: 0.8830\n",
      "Epoch 3/20\n",
      "9000/9000 [==============================] - 8s 863us/step - loss: 0.2850 - accuracy: 0.8805 - val_loss: 0.2520 - val_accuracy: 0.8900\n",
      "Epoch 4/20\n",
      "9000/9000 [==============================] - 8s 872us/step - loss: 0.2425 - accuracy: 0.8976 - val_loss: 0.2278 - val_accuracy: 0.8995\n",
      "Epoch 5/20\n",
      "9000/9000 [==============================] - 8s 874us/step - loss: 0.2099 - accuracy: 0.9101 - val_loss: 0.2141 - val_accuracy: 0.9070\n",
      "Epoch 6/20\n",
      "9000/9000 [==============================] - 8s 881us/step - loss: 0.1834 - accuracy: 0.9234 - val_loss: 0.2067 - val_accuracy: 0.9110\n",
      "Epoch 7/20\n",
      "9000/9000 [==============================] - 8s 868us/step - loss: 0.1639 - accuracy: 0.9345 - val_loss: 0.2040 - val_accuracy: 0.9110\n",
      "Epoch 8/20\n",
      "9000/9000 [==============================] - 8s 862us/step - loss: 0.1502 - accuracy: 0.9404 - val_loss: 0.2086 - val_accuracy: 0.9150\n",
      "10000/10000 [==============================] - 3s 265us/step\n",
      "\n",
      "\n",
      "Test score: 0.27676756706237793\n",
      "Test accuracy: 0.8878499865531921\n",
      "659/659 [==============================] - 0s 756us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8607973125631725"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])\n",
    "\n",
    "score = model.evaluate(x_val, y_val, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "results = model.predict(x_test, batch_size=batch_size, verbose=1)\n",
    "\n",
    "res = (results[:,0]>0.5).astype(int)\n",
    "\n",
    "y_test_ = (y_test[:,0]>0.5).astype(int)\n",
    "\n",
    "roc_auc_score(y_test_, res)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dl-nlp-cnn1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
